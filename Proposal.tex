% Setup
\documentclass[a4paper, titlepage]{article}

% Packages
\usepackage{hyperref}
\usepackage[hyphenbreaks]{breakurl}
\usepackage{soul}
\usepackage[nottoc, numbib]{tocbibind}
\usepackage[dvipsnames]{xcolor}

% Preamble
\hypersetup{hidelinks}

% Document Information
\title{Audio Sample Selection with Generative Adversarial Networks}
\author{James Hill}
\date{}

% Document
\begin{document}

\maketitle
\tableofcontents

\newpage

\section{Introduction}

Generative Adversarial Networks (GANs) \cite{2014arXiv1406.2661G} are a type of generative neural network invented in 2014 and currently the subject of research.
The first of these networks generated novel images and newer models have shown consistently improved results for visual applications.
\newline
\newline
Sound generation has more recently become an area of research for GANs.
Generated audio has practical applications such as natural sounding text-to-speech and realistically dynamic synthesizers for digital audio music production.
Existing models such as WaveGAN \cite{2018arXiv180204208D} successfully capture recorded words from a real data distribution for small-vocabulary speech and randomly generate audio words similar to those in the dataset.
\newline
\newline
A practical audio generator however must respond to an input and generate a corresponding determined output: written words must produce spoken words; sound pitches must produce musical notes.
Methods of determining the output of a GAN have already been introduced and demonstrated for visual applications \cite{2014arXiv1411.1784M}.
This project will attempt to demonstrate output selection with an audio generating GAN.

\newpage

\section{Related Work}

Generative neural network models are powerful methods for making models understand a data distribution.
They can provide important insight into the relationship between input variables and corresponding representations.
They also have practical applications such as image denoising, inpainting, and super resolution, among others \cite{openai_blog_2017}.
\newline
\newline
There are a number of different generative models such as Botzmann machines and their variants, and deep belief networks.
Difficulties arising from using these models include the requirement to use approximations of intractable probabilistic computations, such as Markov Chains.
Another difficulty is the requirement for feedback loops during the generation process.
The GAN model was proposed so as to sidestep these difficulties.
\newline
\newline
Within a GAN model, a generator network is pitted against a discriminator network in a competition that drives both to improve.
In the simplest form, the competition is a zero-sum minimax game where the discriminator is trained to maximize the probability of assigning correct labels to training examples and generated samples, and the generator is trained to minimize this probability.
\newline
\newline
Initial experiments with GANs tended to focus on image generation with audio generation appearing later.
The reasons for this we speculate are: the availability of suitable image datasets and lack of suitable audio datasets when GANs were first subject to experiment; the fact that very small images convey more information to the human subject than very short sounds despite being of similar bit size; and the suitability of images for printed reporting of results.
\newline
\newline
Demand for applications of speech synthesis, generating speech with computers, has largely been based on a so-called concatenative text-to-speech methods and only recently were generative network models introduced with Deep Mind's WaveNet model \cite{waveNetUrl}.
WaveNet was introduced as an autoregressive model operating directly on raw audio waveform \cite{DBLP:journals/corr/OordDZSVGKSK16} (an adaptation can generate musical notes \cite{2017arXiv170401279E}).
\newline
\newline
What advantages are there to use GANs over WaveNet?
\newline
\newline
Generative Adversarial Networks have also been recently applied to the problem of audio generation \cite{2018arXiv180204208D}.
Two strategies were tested: a frequency domain strategy based on naive application of image generating GAN methods to spectograms representing audio; and a time domain strategy operating on raw audio.
The time domain strategy was shown to produce superior results and is the foundation of the WaveGAN model.
The architecture of the WaveGAN model was adapted from a deep convolutional GAN model that was shown to generate convincing images of faces of bedrooms \cite{2015arXiv151106434R}.
\newline
\newline
Early experiments on GANs focussed exclusively on unconditioned generative models with no control on the data being generated.
Conditioning of GANs to produce controllable generated data has since been demonstrated for image and textual tag generation \cite{2014arXiv1411.1784M}.
These first conditional GANs succesfully controlled data generation by conditioning both the generator and discriminator on an extra input layer.
\newline
\newline
A recently proposed alternative approach to conditioning GANs is the Controllable GAN (CGAN) which has been demonstrated with image generation \cite{2017arXiv170800598L}.
CGAN uses a three network model with a classifier/encoder network introduced alongside the generator/decoder and discriminator networks.
The networks conduct a three-player game where the generator attempts to deceive the discriminator and also be classified to the correct corresponding class by the classifier.

\newpage

\section{Aims}

The goal of the current project is to produce a conditioned GAN capable of generating audio samples.
The resulting model will build upon the audio generating architecture of the WaveGAN model and the introduction of a third input layer as demonstrated by the Conditioned and Controllable GAN models.
Generated audio samples from the model will be evaluated against those produced by a baseline unconditioned model with the expectation that the conditioned model will produce inferior samples.

\newpage

\section{Objectives}

Listed below are the proposed milestones for the project:

\begin{itemize}
\item Dataset selection
\item Building the WaveGAN model
  \begin{itemize}
  \item Writing the discriminator
  \item Writing the generator
  \item Training the WaveGAN model
  \end{itemize}
\item Building of conditioned WaveGAN model
  \begin{itemize}
  \item Repurposing components from the WaveGAN model
  \item Integrating an input layer
  \item Training the conditioned WaveGAN model
  \end{itemize}
\item Evaluating results
  \begin{itemize}
  \item Comparing Inception scores
  \item Writing Jupyter report with audio samples
  \item Writing project report
  \end{itemize}
\item Considering further experiments
\end{itemize}

\newpage

\section{Methodology}

\subsection{Dataset Selection}

The \textit{Speech Commands Dataset} \cite{speechcommands} produced by Google consists of thousands of approximately one second long recordings of different individuals pronouncing thirty different english language words.
The \textit{Speech Commands Zero Through Nine} (SC09) dataset is a subset of the \textit{Speech Commands Dataset} used during the development of the WaveGAN model.
The data used in this project will be taken from the \textit{Speech Commands Dataset} so as to maintain a level of consistency with previous experiments.
\newline
\newline
Deep learning experiments can be computationally expensive.
The training of the original WaveGAN model using the SC09 dataset required four days of computation on an NVIDIA P100 GPU.
At the time of writing, NVIDIA P100 GPUs are available for approximately US \$5,500.
Cloud computing services also provide GPUs at varying rates depending on the users requirements; for example, at the time of writing, \href{https://cloud.google.com/gpu/}{Google Cloud} has preemptible NVIDIA P100 GPU computation available at US \$0.73 per hour.
Experiments with the SC09 dataset are both time consuming and have a significant financial cost.
\newline
\newline
Given the limits on time and finances available for this project, the selection of the dataset must reduce the computational cost where possible.
This is achievable by a reduction in the size of the dataset used for the experiment.
There are two aspects of an audio dataset that can be manipulated to reduce the size of the dataset: the number of elements within the dataset and the number of samples used to represent the audio (sample rate).
\newline
\newline
The SC09 dataset has approximately 2,370 elements per recorded number.
With ten numbers included within the dataset, there are approximately 23,700 elements in total.
To demonstrate the principle of conditional selection of generated samples from the WaveGAN model the minimum number of words required is two.
With two words only the model can be conditioned to produce either depending on the input layer.
\newline
\newline
For the purpose of this experiment I propose to create a new \textit{Speech Commands Binary Dataset} as a subset of the \textit{Speech Commands Dataset} used in the original WaveGAN training.
This dataset will contain only elements from the spoken words for the numbers 'zero' and 'one'.
With words of different syllable counts, and with no shared syllables, the two words are easily distinguishable.
The dataset also has consistency with the subject matter by representing the numbers used for digital computation.
With this reduction of the dataset size to one-fifth of that used in the original WaveGAN experiment the computational cost of the experiment should be significantly reduced.
\newline
\newline
Each recording within the \textit{Speech Commands Dataset} is of approximately one second long and at a sample rate of 16,000.
A sample rate of 16,000 consists of 16,000 different numerical measurements.
Humans hear audio as wave cycles measuered in hertz (hz).
A cycle requires at least two samples to be recorded so a recording with a sample rate of 16,000 captures audio within the range from 0hz to 8,000hz.
The most recognizable features of human speech actually lie in the range from 30hz to 4,000hz.
Recordings with a frequency range up to 4,000hz are of lower quality than those with a frequency range up to 8,000hz but are still recognizable as speech; however, they only require a sample rate of 8,000 samples per second.
\newline
\newline
The reduction of the sample rate of audio is 'downsampling'.
Downsampling the dataset elements to 8,000 samples per second will halve the amount of data needed to be passed through the neural network.
The number of convolution layers in the WaveGAN model assumes the division of an input layer of an approximatley 16,000 sample per second audio recording repeatedly by 4 (the stride of the convolution) to layer of length 16.
To repeat this with an input layer of approximately 4,000 samples per second, the model would need to be smaller by one convolution layer.
The reduction of the number of layers within the model may affect the ability of the model to converge as there is a relationship between number of layers and the capability of a model.
\newline
\newline
A \textit{Downsampled Speech Commands Binary Dataset} will therefore be created with a sample rate of approximately 4,000 samples per second and will be tested during the model training to determine whether a smaller model will converge.
If the smaller model succeeds it is possible that the training time will be noticeably reduced.
The removal of the upper convolution layer on a 16,000 sample per second recording would reduce the number of convolution computations by two-thirds.
This reduction of the sample rate is equivalent to using a smaller image size for experimentation with picture generating GANs.
\newline
\newline
The two datasets, \textit{Speech Commands Binary Dataset} and \textit{Downsampled Speech Commands Binary Dataset}, will be produced through a simple Python program.
The LibROSA Python package for music and audio analysis will be imported to enable the manipulation of the .wav files the \textit{Speech Commands Dataset} audio is stored within.

\subsection{Building the WaveGAN Model}

The WaveGAN model will be built with two variations: the first being a recreation of the original WaveGAN; the second being a slightly modified model with one fewer convolution layers to allow for testing the less computationally demanding \textit{Downsampled Speech Commands Binary Dataset}.
Both variations will follow the structure and hyperparameters used within the original WaveGAN as closely as possible.
The code from these baseline models will be reused when writing the conditioned WaveGAN model.
\newline
\newline
The code for these models will be written with Python using the open-source TensorFlow machine learning frameowork published and maintained by Google.
TensorFlow was designed for working with deep neural networks and has functionality such as pre-built convolution and dense layers that can be quickly plugged into a model.
It also includes TensorBoard, a visualization tool that portrays the model graph visually, plots quantitative measures and includes debugging tools.
Although TensorFlow is compatiable with some other languages, such as C++ and Java, the Python API for TensorFlow is ``the most complete and easiest to use.'' \cite{TensorFlowAPI}

\subsubsection{Writing the discriminator}

How accurate does this discriminator need to be?

\subsubsection{Writing the generator}

\subsubsection{Training the WaveGAN model}





Audio is difficult to represent visually so an interactive notebook will also be produced that will allow readers to listen to samples of the audio and compare the results.
This will allow the reader of the final reprot to also make a subjective evaluation of the results.
\newline
\newline
Nonetheless, as shown by Engel et al. it is still possible demonstrate differences between audio visually through the use of 'rainbowgrams'; constant-q transform (CQT) plots of the audio.
These 'rainbowgrams' will therefore also be explored during the evaluation for possible visible differences in the generated and real data samples and will be produced in the final report if they produce interesting comparisons.
\newline
\newline
The Inception Score is another form of evaluation that can be used for comparing generated samples against real data samples and has been used in a number of experiments for generative adversarial networks.
Concerns have been recently raised about the validity of the Inception Score but as there is currently no other comparable alternative it will need to be used in this experiment but with some caution applied to interpretation of the results.
\newline
\newline
Analysis of results will use a statistical language such as Python with extensions or possibly R programming language.

\subsection{Conditioned WaveGAN}

The second model will introduce conditioning into the original model.
The new model will be a WaveGAN but with the introduction of an additional input layer that conditions the randomly generated input.

\subsubsection{Create the Input layer for the Conditioned WaveGAN model}

\subsubsection{Train the Conditioned WaveGAN model}

\subsubsection{Evaluate the performance of the Conditioned WaveGAN model}

Once complete, the model will be evaluated against the Baseline model; it's expected (given the results of the initial Conditioned GAN created by Mirza et al.) that this model will intially produce generated samples less powerful than those created by the baseline WaveGan.
This is acceptable as future exploration of the hyperparameters out of the scope of this experiment would be necessary as to produce better models.
\newline
\newline
In any case, generated samples from both of the conditioned models will be compared against each other and to the samples generated from the baseline WaveGAN.
\newline
\newline
Subjective evaluation is an appropriate approach for generational models and human assessment of random samples by subjects may be used, particularly to obtain preferences between the Conditional WaveGAN and Controlled WaveGAN.
However, due to the requirements of using human samples, having to get sign off for experimentation, having to create an interface to produce the tests, collecting subjects and getting them to respond, it has been decided that this is not possible within the time frame allowed for this project.
Evaluation will therefore be based purely on metrics.
\newline
\newline
Audio samples used in the evaluation of the results will be presented in a Jupyter Notebook format so that the reader may form their own judgement of the quality difference between outputs from each model.

\subsection{Consideration of further work if time permits}

There are a number of possible directions for further exploration of the project if it runs ahead of time.
Which particular direction is chosen will be dependent on factors such as: how much time is left; what does the initial experiment suggest will be either an interesting or feasible direction?
Englargement of the dataset and increasing the sample rate of the audio are both simple changes to the experiment; it is possible that both could be implemented at the same time if it appears feasible.
Increasing the sample rate would be a simple change to make, and would allow for more accurate evaluation of the results, but would not add much to the experiment
Enlargement of the dataset used for the training should be more interesting to evaluate as it is testing a more substantial difficulty; more complex data resulting in a more complex and difficult model.
Attempting to replicate the third model would also produce very interesting results but may be challenging to perform within the project time frame.

\subsubsection{Increasing the Sample rate of audio used}

An alternative would be to increase the sample rate of the audio for a rerun of the experiment.
A rerun with higher quality audio should give a firmer basis for the results of the experiment.
More detailed and clearer audio samples will allow for better evaluation of the results.

\subsubsection{Enlargement of the dataset used for training}

Once all models have been produced and initially evaluated, an enlargement of the number of words used for their training will be considered.
The experiment should first be replicated with the \textit{Downsampled Speech Commands Directions} dataset.
This dataset contains four directional commands from the \textit{Speech Commands Dataset}: 'up', 'down', 'left' and 'right'.
This dataset is double the size of the \textit{Downsampled Speech Commands Binary} dataset so will be used to expand the complexity of the experiment while still being limited enough to restrict potential computational cost.

\subsubsection{Attempting a third model}

Another possibility would be to train a third model: a Controlled GAN trained on the same dataset of spoken 'zero' and 'one'.
If successful then the model will be evaluated for quality against both the baseline model and the Conditioned GAN.
It is again expected that the initial model would be inferior to the baseline model.
However, it will be interesting to compare against the Conditioned WaveGAN as a claim has been made that it should be a superior method for training GANs with conditioning.
However, a difficulty with attempting this third model is that the paper in which it is described is fairly scanty on details of the architecture.
\newline
\newline
Evaluation of the results would then be required on the final sets of generated samples: between the Conditional WaveGAN and baseline WaveGAN; between the Controlled WaveGAN and baseline WaveGAN; and between the Conditional WaveGAN and Controlled WaveGAN.

\newpage

\section{Potential Difficulties}

The initial WaveGAN model was run for 4 days on an NVIDIA P100 GPU before converging (on sale for upwards of £6k at the time of writing).
It is possible that this level of processing power will not be available.
Downsampling of the original dataset is one method to ameliorate this problem.
\newline
\newline
However, the experiment can prove the concept of a conditional WaveGAN without using the full dataset used for the creation of the initial WaveGAN.
The initial dataset for WaveGAN used words for numbers 'zero' through 'nine'.
To reduce the computational requirements a lower count of words may be chosen for the experiment.
Two words, 'zero' and 'one' can be used to prove that the models work and should allow for comparison of generated sample quality.
If the models are successful and if it seems sensible then the number of words used for the training of the models may be increased.
\newline
\newline
Secondly, in order to reduce the amount of data to be processed, the experiment may also use downsampled versions of the dataset.
Downsampling of audio reduces the definition and perceived quality (at least at the lower sample rates that might be needed in this case) but will reduce the amount of data to be trained in the networks.
This shouldn't limit the validity of the experiment because perceived audio quality is not a key measurement; the difference in quality between models is of prime importance.

\newpage

\section{Time Plan}

Write here a detailed and realistic plan of how long each section of the project is likely to take.

\newpage

\section{Finance}

The financing of a deep learning experiment must be taken into consideration as the cost of computation can be expensive.
For example, the training of the original WaveGAN network by Donahue et al. required four days of calculation on an NVIDIA P100 GPU before convergence.
At the time of writing, an NVIDIA P100 GPU is on sale for approximatley \$5,500 on Amazon.com.
It is therefore essential to bring down the potential cost of the experiment.
\newline
\newline
The creation of downsampled datasets for running the experiment is the first method of reducing the overall cost.
Downsampling can reduce the datasets to between 3/8s and 1/2 of the original size and should result in a similar reduction of the computation.
\newline
\newline
The reduction of the number of modes of the data from ten in the original \textit{Speech Commands Zero Through Nine} dataset to two in the proposed \textit{Downsampled Speech Commands Binary} dataset and four in the \textit{Downsampled Speech Commands Directions} will also decrease the cost of the experiment.
The two mode dataset could reduce the final computation time to one fifth of the original undertaken by Donahue et al. while the four mode dataset would be two fifths of the computation time.
\newline
\newline
With the proposed downsampling rates and limited datasets, the final time requirement for training of the initial WaveGAN on a similarly powered GPU could potentially be brought down to between 3/40ths and 1/5th.
\newline
\newline
A GPU is available for use in this experiment: it is an NVIDIA GPU that can be used with CUDA (Compute Unified Device Architecture) with a compute capability of 2.1 (compared to a compute capability of 6.0 for an NVIDIA P100 GPU.
This is a substantially less powerful GPU but may be useful for at least running trials and the training of simpler models such as the discriminator networks for the variations on the WaveGAN model.
\newline
\newline
Where the available GPU is insufficient, the experiment can be performed on cloud GPU.
For example, at the time of writing, \href{https://cloud.google.com/gpu/}{Google Cloud} has NVIDIA P100 GPUs available at \$0.73 for preemptible instances that can be used for batch based processing.
There are other services available such as Amazon Web Services: the choice of service will need to consider the ease of use of the service, compatibility with the tools being used, and the relative pricing at time of running the experiment.

\newpage
% Bibliography
\bibliography{Bibliography}
\bibliographystyle{ieeetr}

% Wrapup
\end{document}
