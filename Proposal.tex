% Setup
\documentclass{article}

% Packages
\usepackage{hyperref}
\usepackage[hyphenbreaks]{breakurl}

% Document Information
\title{Sample Selection with WaveGAN}
\author{James Hill}
\date{}

% Document
\begin{document}

\maketitle

\section{Introduction}

Generated audio has potential practical applications such as text-to-speech and digital audio music production.
A practical audio generator would need to respond to an input and generate a corresponding output: written words could produce spoken words; digital audio commands (midi) could produce musical notes as played by a musical instrument.
\newline
\newline
Generative Adversarial Networks (Goodfellow et al., 2014) are unsupervised algorithms that have produced consistently improved results for image generation.
Recently an application of GANs to unsupervised audio generation introduced the WaveGAN model (Donahue et al., 2018) which was shown to successfully capture (and generate) words from a real data distribution for small-vocabulary speech.
\newline
\newline
In an unsupervised generative model such as the initial WaveGAN there is no control on the modes of data (output) being generated.
There are now two proposed models for controlling the output of a GAN: Conditional GAN and Controllable GAN but it has not yet been demonstrated which is most capable of giving the best results.
\newline
\newline
The goal of the project is to produce the best possible WaveGAN model capable of generating controlled outputs as using either of the two methodologies for achieving this.
An essential part of the project is to determine through experimentation which of these will produce the best results.

\section{Related Work}

Generative Adversarial Networks were recently tested for audio applications for the first time by Donahue et al.
Two strategies were tested: a frequency domain strategy and a time domain strategy.
The frequency domain strategy was based on a naive application of image generating GAN methods to spectograms.
The time domain strategy was an approach that operated on the raw audio; this was the basis of the WaveGAN model.
WaveGAN was adapted from a Deep Convolutional GAN model (Radford et al., 2016) that has been shown to produce convincing generation of scenes.
\newline
\newline
Conditioning of GANs to produce controllable generated output was first demonstrated on image generation applications (Mirza et al., 2014).
Mirza et al. were able to succesfully control the output of a single GAN trained generative model by inserting 1-hot vectors alongside random input into the generator.
\newline
\newline
A recently proposed and tested approach is the Controllable Generative Adversarial Network (Lee et al., 2017).
CGAN uses a three network model with a classifier network introduced alongside the regular generator and discriminator.
Examples of compared output from both conditional and controllable GANs are shown in the paper but no rigorous evaluation is offered.

\section{Objectives}

The project is split into stages: the main goal is to produce all three models with the maximum possible number of data modes (number of spoken words trained); equivalent to stage 3.4 below.

\subsection{Baseline WaveGAN}

Initially, a replica of the initial unsupervised WaveGAN will be trained.
It will use near identical structure and hyperparameters as the model produced by Donahue et al.
Unlike the original model it will be trained only on two of the numbers from the dataset: zero and one.
Zero and one are chosen as they are the binary numbers, have different sounds, and also differ in number of syllables.
\newline
\newline
The process of creating this initial model will allow for familiarization of the process of producing a mode from code to runtime; any difficulties encountered should be encountered and overcome within a simplified setting. 

\subsection{Conditioned WaveGAN}

The second model will introduce conditioning into the original model.
The new model will be a WaveGAN but with the introduction of an additional input layer that conditions the randomly generated input.
This model will also be trained as a simplified version using only ``zero'' and ``one''.
\newline
\newline
Once complete, the model will be evaluated against the Baseline model; it's expected (given the results of the initial Conditioned GAN created by Mirza et al.) that this model will intially produce generated samples less powerful than those created by the baseline WaveGan.
This is acceptable as future exploration of the hyperparameters out of the scope of this experiment would be necessary as to produce better models.

\subsection{Controlled WaveGAN}

The third model will be a Controlled GAN trained on the same dataset of spoken ``zero'' and ``one''.
If successful then the model will be evaluated for quality against both the baseline model and the Conditioned GAN.
It is again expected that the initial model would be inferior to the baseline model.
However, it will be interesting to compare against the Conditioned WaveGAN as a claim has been made that it should be a superior method for training GANs with conditioning.

\subsection{Enlarged Models}

Once all models have been produced and initially evaluated, an enlargement of the number of words used for their training will be considered.
Ideally, the full set of spoken numbers from ``zero'' through ``nine'' would be used but this may not be possible with the available computational power.
A decision will need to be made based on experimentation an dinformed by previous experience of the model training.
\newline
\newline
If downsampling was used to increase the speed of model training previously, an increase in sample size may also be considered at this point.
Ideally the sample size of the original dataset without any downsampling would be used.
Again, experimentation and judgement based on previous experience will be necessary in order to decide on the best approach.
\newline
\newline
Evaluation of the results would then be required on the final sets of generated samples: between the Conditional WaveGAN and baseline WaveGAN; between the Controlled WaveGAN and baseline WaveGAN; and between the Conditional WaveGAN and Controlled WaveGAN.

\subsection{Further Work: Prototype Synthesizer}

If time is available after the completion and evaluation of the three models then further work can encompass testing the creation of a prototype synthesizer.
A synthesizer is an electronic or digital audio generator that can be controlled by pitch (most commonly by an electronic keyboard but other methods are available).
Using the best of either the Conditional WaveGAN or Controlled WaveGAN models, training will take place over a subset of the NSynth dataset (Engel et al, 2017).
\newline
\newline
The goal will be the training of a model that can be controlled by musical pitch.
The resulting sounds should likely be limited to a relatively small musical range (perhaps a single octave of 12 notes) and a single type or family of musical instruments (one with a relatively simple frequency response).

\section{Evaluation}

Evaluation of a Conditional WaveGAN that can generate words will be by comparing the quality of the reproduced sounds against those of the original WaveGAN.
If downsampling has taken place, then an equivalent downsampling of the generated sounds from the original WaveGAN may also take place in order that comparisons are fair.
This is acceptable as the comparison is between sets of generated samples and not against the absolute standard of real data.

\section{Potential Hazards}

The initial WaveGAN model was run for 4 days on an NVIDIA P100 GPU before converging (on sale for upwards of Â£6k at the time of writing).
It is possible that this level of processing power will not be available.
\newline
\newline
Firstly, the experiment can prove the concept of a conditional WaveGAN without using the full dataset used for the creation of the initial WaveGAN.
The initial dataset for WaveGAN used words for numbers ``zero'' through ``nine''.
To reduce the computational requirements a lower count of words may be chosen for the experiment.
Two words, ``zero'' and ``one'' can be used to prove that the models work and should allow for comparison of generated sample quality.
If the models are successful and if it seems sensible then the number of words used for the training of the models may be increased.
\newline
\newline
Secondly, in order to reduce the amount of data to be processed, the experiment may also use downsampled versions of the dataset.
Downsampling of audio reduces the definition and perceived quality (at least at the lower sample rates that might be needed in this case) but can reduce the .
However, this should not limit the validity of the experiment because the perceived audio quality is not a key measurement; the difference in quality between models is of prime importance.

\section{Datasets}

For the concept model, the dataset \textit{Speech Commands Zero Through Nine} (SC09) introduced by Donahue et al. will be used.
This is a subset of the \textit{Speech Commands Dataset} (Warden, 2017) containing only the words for numbers ``zero'' through ``nine''.
There are 1850 utterances of each word.
Each utterance is one second in length.
This results in 5.3 hours of speech.
\newline
\newline
NSynth is a large-scale dataset of musical notes (Engel et al., 2017).
It contains ~300k four-second annotated notes samples at 16kHz from ~1k harmonic music instruments.

\section{Technologies}

The models will be designed with the TensorFlow machine intelligence package for Python programming language.
Analysis of results will use a statistical language such as Python with extensions or possibly R programming language.
Audio samples used in the evaluation of the results will be presented in a Jupyter Notebook format so that the reader may form their own judgement of the quality difference between outputs from each model.

\section{Future Work}

Further experimentation with a WaveGAN synthesizer would be possible by also introducing velocity as a second dimension of the conditional input (velocity relates to the force with which a musical note is played on an instrument).
This would require the model to learn a further dimension of the sounds and would be a prototype for more realistic synthesizers.

\section{References}

\begin{enumerate}

\item
  Donahue, Chris, McAuley, Julian, Puckette, Miller. Synthesizing Audio with Generative Adversarial Networks.
\item
  Engel, Jesse, Resnick, Cinjon, Roberts, Adam, Dieleman, Sander, Eck, Douglas, Simonyan, Karen, and Norouzi, Mohammad. Neural audio synthesis of musical notes with WaveNet autoencoders. In ICML, 2017.
\item
  Goodfellow, Ian, Pouget-Abadie, Jean, Mirza, Mehdi, Xu, Bing, Warde-Farley, David, Ozair, Sherjil, Courville, Aaron, and Bengio, Yoshua. Generative adversarial networks. In NIPS, 2014.
\item
  Lee, Minhyeok, Seok, Junhee. Controllable Generative Adversarial Network. 2017.
\item
  Radford, Alec, Metz, Luke, and Chintala, Soumith. Unsupervised representation learning with deep convolutional generative adversarial networks. In ICLR, 2016.
\item
  Warden, Pete. Speech commands dataset.
\burl{https://research.googleblog.com/2017/08/launching-speech-commands-dataset.html}, 2017.
Accessed:

\end{enumerate}

% Wrapup
\end{document}
