% Setup
\documentclass[a4paper, dvipsnames, titlepage]{article}

% Packages
\usepackage{amsfonts}
\usepackage[toc, page]{appendix}
\usepackage[hyphenbreaks]{breakurl}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage[round]{natbib}
\usepackage{pdflscape}
\usepackage{pgfgantt}
\usepackage{ragged2e}
\usepackage{soul}
\usepackage{textgreek}
\usepackage[nottoc, numbib]{tocbibind}
\usepackage{xcolor}

% Preamble
\hypersetup{hidelinks}

% Document Information
\title{Audio Sample Selection with Generative Adversarial Networks}
\author{James Hill}
\date{}

% Document
\begin{document}

\maketitle
\tableofcontents

\newpage

\section{Introduction}

Generative Adversarial Networks (GANs) \citep{2014arXiv1406.2661G} are a type of generative neural network invented in 2014.
The first of these networks generated novel images and newer models have shown consistently improved results for visual applications.
\newline
\newline
Audio generation has more recently become an area of research for GANs.
Generated audio has practical applications such as natural sounding text-to-speech and realistical synthesizers for digital audio music production.
Existing models such as WaveGAN \citep{2018arXiv180204208D} successfully capture recorded words from a real data distribution for small-vocabulary speech and randomly generate audio words similar to those in the dataset.
\newline
\newline
A practical audio generator however must respond to input and generate corresponding determined outputs: written words producing spoken words; sound pitch producing musical notes.
Methods of determining the output of GANs have already been demonstrated for visual applications \citep{2014arXiv1411.1784M}.
This project will demonstrate output selection with an audio generating GAN.

\newpage

\section{Related Work}

Generative neural network models are powerful methods for making models understand a data distribution.
They can provide important insight into the relationship between input variables and corresponding representations.
They also have practical applications such as image denoising, inpainting, and super resolution, among others \citep{openai_blog_2017}.
\newline
\newline
There are a number of different generative models such as Botzmann machines and their variants, and deep belief networks.
Difficulties arising from using these models include the requirement to use approximations of intractable probabilistic computations, such as Markov Chains.
Another difficulty is the requirement for feedback loops during the generation process.
The GAN model was proposed so as to sidestep these difficulties.
\newline
\newline
Within a GAN model, a generator network is pitted against a discriminator network in a competition that drives both to improve.
In the simplest form, the competition is a two-player zero-sum minimax game where the discriminator is trained to maximize the probability of assigning correct labels to training examples and generated samples, and the generator is trained to minimize this probability.
\begin{equation}
  \min_{G} \max_{D} V(D,G) = \mathbb{E}_{x \sim P_\mathrm{data}(x)}[\mathrm{log}\,D(x)] + \mathbb{E}_{z \sim p_z(z)}[\mathrm{log} (1 - D(G(z)))]
\end{equation}
\newline
\newline
Initial experiments with GANs tended to focus on image generation with audio generation appearing later.
The reasons for this we speculate are: the availability of suitable image datasets and lack of suitable audio datasets when GANs were first subject to experiment; the fact that very small images convey more information to the human subject than very short sounds despite being of similar bit size; and the suitability of images for printed reporting of results.
\newline
\newline
Demand for applications of speech synthesis, generating speech with computers, has largely been based on a so-called concatenative text-to-speech methods and only recently were generative network models introduced with Deep Mind's WaveNet model \citep{waveNetUrl}.
WaveNet was introduced as an autoregressive model operating directly on raw audio waveforms \citep{DBLP:journals/corr/OordDZSVGKSK16} (an adaptation can generate musical notes \citep{2017arXiv170401279E}).
\newline
\newline
Generative Adversarial Networks have also been recently applied to the problem of audio generation \citep{2018arXiv180204208D}.
Two strategies were tested: a frequency domain strategy based on naive application of image generating GAN methods to spectograms representing audio; and a time domain strategy operating on raw audio.
The time domain strategy was shown to produce superior results and is the foundation of the WaveGAN model.
The architecture of the WaveGAN model was adapted from a deep convolutional GAN model that was shown to generate convincing images of faces of bedrooms \citep{2015arXiv151106434R}.
\newline
\newline
The architecture of the WaveNet model allows for audio samples of arbitrary length whereas the the WaveGAN model with the current architecture only produces audio samples of a preset length.
Donahue et al. however report that the WaveNet model is not competitive with the WaveGAN model and that work is ongoing on extending the WaveGAN model for arbitrary length audio.
\newline
\newline
Early experiments on GANs focussed exclusively on unconditioned generative models with no control on the data being generated.
Conditioning of GANs to produce controllable generated data has since been demonstrated for image and textual tag generation \citep{2014arXiv1411.1784M}.
These conditional GANs succesfully controlled data sample generation by conditioning both the generator and discriminator on an extra input layer.
\begin{equation}
  \min_{G} \max_{D} V(D,G) = \mathbb{E}_{x \sim P_\mathrm{data}(x)}[\mathrm{log}\,D(x|y)] + \mathbb{E}_{z \sim p_z(z)}[\mathrm{log} (1 - D(G(z|y)))]
\end{equation}
\newline
\newline
A recently proposed alternative approach to conditioning GANs is the Controllable GAN (CGAN) which has been demonstrated with image generation \citep{2017arXiv170800598L}.
CGAN uses a three network model with a classifier/encoder network introduced alongside the generator/decoder and discriminator networks.
The networks conduct a three-player game where the generator attempts to deceive the discriminator and also be classified to the correct corresponding class by the classifier.

\newpage

\section{Aims}

The goal of the current project is to produce a conditioned GAN capable of generating audio samples.
The resulting model will build upon the audio generating architecture of the WaveGAN model and the introduction of a third input layer as demonstrated by the Conditioned and Controllable GAN models.
Generated audio samples from the model will be evaluated against those produced by a baseline unconditioned model with the expectation that the conditioned model will produce inferior samples.

\newpage

\section{Objectives}

Listed below are the proposed milestones for the project:

\begin{itemize}
\item Dataset selection
\item Building the WaveGAN model
  \begin{itemize}
  \item Writing the discriminator
  \item Writing the generator
  \item Training the WaveGAN model
  \end{itemize}
\item Building the conditioned WaveGAN model
  \begin{itemize}
  \item Integrating an input layer
  \item Training the conditioned WaveGAN model
  \end{itemize}
\item Evaluating results
  \begin{itemize}
  \item Comparing Inception scores
  \item Nearest neighbour comparison
  \item Writing project report
  \item Writing Jupyter report with audio samples
  \end{itemize}
\item Considering further experiments
  \begin{itemize}
  \item Increasing the sample rate of training data
  \item Enlargement of the training dataset
  \item Training a third model
  \end{itemize}
\end{itemize}

\newpage

\section{Methodology}

\subsection{Dataset Selection}

The \textit{Speech Commands Dataset} \citep{speechcommands} produced by Google consists of thousands of approximately one second long recordings of different individuals pronouncing thirty different english language words.
The \textit{Speech Commands Zero Through Nine} (SC09) dataset is a subset of the \textit{Speech Commands Dataset} used during the development of the WaveGAN model.
The data used in this project will be taken from the \textit{Speech Commands Dataset} so as to maintain a level of consistency with previous experiments.
\newline
\newline
Deep learning experiments can be computationally expensive.
The training of the original WaveGAN model using the SC09 dataset required four days of computation on an NVIDIA P100 GPU.
At the time of writing, NVIDIA P100 GPUs are available for approximately US \$5,500.
Cloud computing services also provide GPUs at varying rates depending on the users requirements; for example, at the time of writing, \href{https://cloud.google.com/gpu/}{Google Cloud} has preemptible NVIDIA P100 GPU computation available at US \$0.73 per hour.
Experiments with the SC09 dataset are both time consuming and have a significant financial cost.
\newline
\newline
Given the limits on time and finances available for this project, the selection of the dataset must reduce the computational cost where possible.
This is achievable by a reduction in the size of the dataset used for the experiment.
There are two aspects of an audio dataset that can be manipulated to reduce the size of the dataset: the number of elements within the dataset and the number of samples used to represent the audio (sample rate).
\newline
\newline
The SC09 dataset has approximately 2,370 elements per recorded number.
With ten numbers included within the dataset, there are approximately 23,700 elements in total.
To demonstrate the principle of conditional selection of generated samples from the WaveGAN model the minimum number of words required is two.
With two words only the model can be conditioned to produce either depending on the input layer.
\newline
\newline
For the purpose of this experiment I propose to create a new \textit{Speech Commands Binary Dataset} as a subset of the \textit{Speech Commands Dataset} used in the original WaveGAN training.
This dataset will contain only elements from the spoken words for the numbers 'zero' and 'one'.
With words of different syllable counts, and with no shared syllables, the two words are easily distinguishable.
The dataset also has consistency with the subject matter by representing the numbers used for digital computation.
With this reduction of the dataset size to one-fifth of that used in the original WaveGAN experiment the computational cost of the experiment should be significantly reduced.
\newline
\newline
Each recording within the \textit{Speech Commands Dataset} is of approximately one second long and at a sample rate of 16,000.
A sample rate of 16,000 consists of 16,000 different numerical measurements.
Humans hear audio as wave cycles measuered in hertz (hz).
A cycle requires at least two samples to be recorded so a recording with a sample rate of 16,000 captures audio within the range from 0hz to 8,000hz.
The most recognizable features of human speech actually lie in the range from 30hz to 4,000hz.
Recordings with a frequency range up to 4,000hz are of lower quality than those with a frequency range up to 8,000hz but are still recognizable as speech; however, they only require a sample rate of 8,000 samples per second.
The subjective quality of the audio will not be invalidate the results of the experiment as evaluation will rely largely on metrics rather than human listening.
\newline
\newline
The reduction of the sample rate of audio is 'downsampling'.
Downsampling the dataset elements to 8,000 samples per second will halve the amount of data needed to be passed through the neural network.
The number of convolution layers in the WaveGAN model assumes the division of an input layer of an approximatley 16,000 sample per second audio recording repeatedly by 4 (the stride of the convolution) to layer of length 16.
To repeat this with an input layer of approximately 4,000 samples per second, the model would need to be smaller by one convolution layer.
The reduction of the number of layers within the model may affect the ability of the model to converge as there is a relationship between number of layers and the capability of a model.
\newline
\newline
A \textit{Downsampled Speech Commands Binary Dataset} will therefore be created with a sample rate of approximately 4,000 samples per second and will be tested during the model training to determine whether a smaller model will converge.
If the smaller model succeeds it is possible that the training time will be noticeably reduced.
The removal of the upper convolution layer on a 16,000 sample per second recording would reduce the number of convolution computations by two-thirds.
This reduction of the sample rate is equivalent to using a smaller image size for experimentation with picture generating GANs.
\newline
\newline
The two datasets, \textit{Speech Commands Binary Dataset} and \textit{Downsampled Speech Commands Binary Dataset}, will be produced through a simple Python program.
The LibROSA Python package for music and audio analysis will be imported to enable the manipulation of the .wav files the \textit{Speech Commands Dataset} audio is stored within.

\subsection{Building the WaveGAN Model}

The WaveGAN model will be built with two variations: the first being a recreation of the original WaveGAN; the second being a slightly modified model with one fewer convolution layers to allow for testing the less computationally demanding \textit{Downsampled Speech Commands Binary Dataset}.
Both variations will follow the structure and hyperparameters used within the original WaveGAN as closely as possible.
The code from these baseline models will be reused when writing the conditioned WaveGAN model.
\newline
\newline
The code for these models will be written with Python using the open-source TensorFlow machine learning frameowork published and maintained by Google.
TensorFlow was designed for working with deep neural networks and has functionality such as pre-built convolution and dense layers that can be quickly plugged into a model.
It also includes TensorBoard, a visualization tool that portrays the model graph visually, plots quantitative measures and includes debugging tools.
Although TensorFlow is compatiable with some other languages, such as C++ and Java, the Python API for TensorFlow is ``the most complete and easiest to use.'' \citep{TensorFlowAPI}

\subsubsection{Writing the Discriminator}

The WaveGAN discriminator will take advantage of TensorFlow objects for one-dimensional convolution, denses layers and reshaping.
A custom layer will be written to account for the 'Phase Shuffle' operation included in the original WaveGAN model.
This 'Phase Shuffle' operation randomly shifts the phase of each layer's activations before input to the next layer; this shift accounts for artifacts in the data that are prouced by the convolution procedure.
Two discriminators will be written; one that can accept 16,000 sample per second audio and another that can accept 8,000 sample per second audio.

\subsubsection{Writing the Generator}

The WaveGAN generator will be written using the TensorFlow objects for dense layers and reshaping.
Currently the TensorFlow object for the one-dimensional transposed convolution has not been exposed through the regular API because it is still being tested.
The function is however available through the TensorFlow 'contrib' module (which is used to expose volatile or experimental code).
The only remaining failed tests for this function are related to the MAC operating system and it should not interfere with this experiment which is being run on Linux Ubuntu systems.
Two generators will be written; one that generates 16,000 sample per second audio and another that generates 8,000 sample per second audio.

\subsubsection{Training the WaveGAN model}

The training of the intergrated WaveGAN model will begin with an attempt to train with the \textit{Downsampled Speech Commands Binary Dataset}.
If attempts to train with the downsampled audio and smaller network fail then the experiment will continue with training on the \textit{Speech Commands Binary Dataset} which will be a near replica of the original WaveGAN model.
\newline
\newline
Hyperparameters chosen for the model will match as closely as possible those chosen for the original WaveGAN model.
The model used for the downsampled dataset may need some slight adjustment and if intitial training fails to converge some experimentation with the parameters will be considered before switching to replica model.
\newline
\newline
An NVIDIA GPU will be available for this experiment and initial tests of the training will take place on that device using the NVIDIA Compute Unified Device Architecture (CUDA) for GPU acceleration.
The available GPU is an older card with a CUDA compute capability of 2.1 compared to the compute capability of 6.0 for the NVIDIA Tesla P100 GPU used in the original WaveGAN experiment.
This GPU will be useful for running some tests upon and debugging the WaveGAN model but only experimentation will determine whether it is suitable to train even the minimal dataset proposed for this project.
\newline
\newline
If the available GPU is insufficient for training then the experiment will be continued on a cloud GPU service such as those provided by Google Cloud or Amazon Web Services.
The choice of service may be price dependent as the cost of computation changes over time.
At the time of writing the costs are reasonable for the purpose of this experiment; for example, \href{https://cloud.google.com/gpu/}{Google Cloud} has NVIDIA Tesla P100 GPUs available at US \$0.73 per hour for preemptible instances that can be used for batch processing.
Even if the computation needs to be hurried, the current cost for on demand GPU computation on the same GPUs at Google is only US \$1.46 per hour.
\newline
\newline
Given the reduction in the data being processed to one fifth the size of the original \textit{Speech Commands Dataset}; it is possible that the training time will be less than one day (compared four days required for training the original WaveGAN).
With a further reduction of the number of computations possible through downsampling it is possible that the training time will be reduced to a few hours.
(This assumes a linear relationship between the size of the dataset, count of samples, and the training time; the true relationship is unlikely to be so simple).
It seems likely that the budget for this project will not exceed US \$200 even with a few failed attempts at training at on demand pricing; this is a reasonable and affordable amount for this project.

\subsection{Building the Conditioned WaveGAN Model}

The conditioned WaveGAN model will be an adaptation of the successfully trained WaveGAN model: this will be the model with downsampled 8,000 sample per second audio as input if that is successful; otherwise it will be the model with 16,000 sample per second audio.

\subsubsection{Integrating an Input Layer}

The code written for the successfully trained WaveGAN implemention will be copied over to the conditioned WaveGAN model.
The model will then be adapted to include an input layer that feeds class labels of the audio samples encoded as one-hot vectors into both the generator and discriminator.
The generator will require the existing prior input noise to be combined with the conditioning input layer before the first transposed convolution layer.
The discriminator will require conditioning input layer to be combined with the data element being tested before convolution begins.

\subsubsection{Training the Conditioned WaveGAN Model}

The training of the conditioned WaveGAN model will begin by following the method used successfully in the training of the baseline WaveGAN model trained for this project.
Results of previous experimentation with a conditioned GAN suggest that the architecture of the discriminator is not a critical factor so the discriminator within this model is unlikely to require any significant changes \citep{2014arXiv1411.1784M}.
The architecture of the generator was not addressed within the report on that experiment so training must begin with the assumption that the generator is modified as little as possible.

\subsection{Evaluating Results}

The primary outcome of the project is the ability of the conditioned WaveGAN to reproduce the correct word given a particular class label as an input.
Inspection of the generated samples by the experimenter is a first step towards the evaluation of generative models and therefore requires a subjective evaluation of the generated data; this will even take place during training as the result of an evaluation may demand changes to the model or hyperparameters.
A poor model may however generate convincing samples by copying elements of the data; human evaluation is therefore not sufficient to confirm the quality of the model.
\newline
\newline
A second outcome is the quality of the generated samples from the conditioned model in comparison to both the baseline model and the training data.
Based on the previous experiment with conditioning a GAN, the conditioned model is not expected to generated outputs to the same quality as the unconditioned GAN.
Measurement of the difference in quality will however give an objective basis for any claim that the conditioned model is generating distinguishable samples.
Comparison against the training data will also support any claim that the samples being generated match the correct class that has been input into the generator.
\newline
\newline
The evaluation of the quality of generated samples in the best case is performed by experimental subjects who are not aware of the source of the samples.
These forms of evaluation are not always possible due to time and budget constraints: the need for ethical sign-off by a research institute, the design and production of a testing interface, and the need to gather subjects for testing in either a controlled environment or through a service such as Amazon's Mechanical Turk all place significnt demands on the experimenter.
Given limited time and budget, it has been decided that evaluation by third-party subjects is outside the scope of this project.

\subsubsection{Comparing Inception scores}

The \textit{inception score} is a measurement for generated images that has been shown to correspond well with human judgement.
Following the example of the paper introducing the original WaveGAN model, an inception classifier will be trained with the training dataset and used to evaluate the generated audio.
The inception classifier will evaluate the audio from both generators represented as Fourier transformed frequency spectrum images.

\subsubsection{Nearest Neighbour Comparison}

Nearest neighbour comparison will be used to confirm that the generators are not simply reproducing the training data; again the experiment follows the example of the original WaveGAN paper.
This evaluation will measure the average Euclidean distance between the same frequency spectrum image representation used for inception score comparison.
Two metrics will be measured via nearest neighbour comparison: firstly, the diversity amongst the generated samples; secondly, the distance between generated samples and their nearest neighbour in the training data.

\subsubsection{Writing the Project Report}

The final written report is a substantial work that needs to be considered during the planning of this project.
While some of the report will be written in an ongoing manner, the results of the report can only be included after the full evaluation.
A block of time will therefore be included for the purpose of finishing the report.
The report will be written with LaTeX so as to allow for version control and to produce high-quality formatting and visuals. 

\subsubsection{Writing Jupyter Report with Audio Samples}

Due to the difficulty for humans in comparing audio through visual representations, a selection of generated samples from both models will be presented alongside selected training data samples in an interactive Jupyter notebook.
The reader will therefore able to subjectively evaluate the results of the project.

\subsection{Considering Further Experiments}

If the project runs ahead of schedule there are a number of possible further experiments that may be considered in order to bolster the results.
Experience of the time and financial cost of training the initial models will inform whether further experiments are viable within the scope of this project; the choice of further experimentation will be dependent on the time available and the benefits that may result from the extra data.

\subsubsection{Increasing the Sample Rate of Training Data}

If the experiment had been successfully completed with the \textit{Downsampled Speech Commands Binary Dataset} a rerun with higher quality audio from the \textit{Speech Commands Binary Dataset} should be considered.
The discriminator and generator models will already be available with the correct architecture as they will be produced during the initial writing of the baseline WaveGAN model.
More detailed and clearer audio samples will allow for a better evaluation and clearer comparison to the results of the original WaveGAN experiment.

\subsubsection{Enlargement of the Training Dataset}

Training of the models with a larger dataset consisting of more than two words would introduce further complexity into the models.
A consistent themed subset of the \textit{Speech Commands Dataset} dataset is a proposed \textit{Speech Commands Directions Dataset} containing recordings of the words for 'up', 'down', 'left' and 'right'.
This dataset is double the size of the \textit{Speech Commands Binary Dataset} yet still restricted to reduce the potential computational cost; it may be downsampled for further cost effectiveness.

\subsubsection{Training a Third Model}

A final possibility is to train a third model based on the controllable GAN (CGAN).
This would require modifying the baseline WaveGAN model to include a classifier/encoder network.
This would be a challenging and time-consuming addition to the experiment and the difficulty is compounded by a lack of details about the architecture within the paper that originally introduced this model.
If achievable however the evaluation could measure the quality of samples from the conditional GAN against the controllable GAN.

\newpage

\section{Time Plan}

Following is a Gantt Chart showing the planned timeline for the project:
\newline
\newline

\begin{ganttchart}[
  vgrid,
  hgrid,
  x unit=2cm,
  y unit title=0.7cm,
  y unit chart=0.7cm,
  progress label text={\quad\pgfmathprintnumber[precision=0, verbatim]{#1}\%},
  milestone label font=\tiny,
  group label font=\tiny,
  title label font=\tiny,
  bar label node/.style={text width=3cm, align=right, font=\scriptsize\RaggedLeft, anchor=east},
  milestone label node/.style={text width=2cm, align=right, font=\scriptsize\RaggedLeft, anchor=east},
  group label node/.style={text width=3cm, align=right, font=\scriptsize\RaggedLeft, anchor=east},
  compress calendar,
  time slot format=isodate
  ]
  {2018-06-01}
  {2018-09-30}
  \gantttitlecalendar{year, month=name} \\
  \ganttgroup{Dataset}{2018-06-15}{2018-06-22} \\
  \ganttgroup{WaveGAN}{2018-06-23}{2018-07-22} \\
  \ganttbar{Write discriminator}{2018-06-23}{2018-06-30} \\
  \ganttlinkedbar{Write generator}{2018-07-01}{2018-07-07} \\
  \ganttlinkedbar{Train WaveGAN}{2018-07-08}{2018-07-22} \\
  \ganttgroup{Conditioned WaveGAN}{2018-07-23}{2018-08-14} \\
  \ganttbar{Input layer}{2018-07-23}{2018-08-31} \\
  \ganttlinkedbar{Train conditioned WaveGAN}{2018-08-01}{2018-08-14} \\
  \ganttgroup{Evaluation}{2018-08-15}{2018-09-30} \\
  \ganttbar{Inception scores}{2018-08-15}{2018-08-22} \\
  \ganttlinkedbar{Nearest neighbour}{2018-08-23}{2018-08-30} \\
  \ganttlinkedbar{Project report}{2018-09-01}{2018-09-21} \\
  \ganttlinkedbar{Jupyter report}{2018-09-22}{2018-09-30}
\end{ganttchart}

\newpage

% Bibliography
\bibliography{Bibliography}
\bibliographystyle{plainnat}

\newpage

% Appendix
\begin{appendices}

  \section{Model Architecture Tables}

  \begin{table}[h]
    
    \caption{\label{tab:table-name} WaveGAN Generator}
    
    \begin{center}
      \begin{tabular}{ l | l | l}
        
        Operation & Kernel Size & Output Shape \\
        \hline
        Input {\it z} $\sim$ Uniform(-1, 1) & & ({\it n}, 100) \\
        Dense 1 & (100, 256{\it d}) & ({\it n}, 256{\it d}) \\
        Reshape & & ({\it n}, 16, 16{\it d}) \\
        ReLU & & ({\it n}, 16, 16{\it d}) \\
        Trans Conv1D (Stride=4) & (25, 16{\it d}, 8{\it d}) & ({\it n}, 64, 8{\it d}) \\
        ReLU & & ({\it n}, 64, 8{\it d}) \\
        Trans Conv1D (Stride=4) & (25, 8{\it d}, 4{\it d}) & ({\it n}, 256, 4{\it d}) \\
        ReLU & & ({\it n}, 256, 4{\it d}) \\
        Trans Conv1D (Stride=4) & (25, 4{\it d}, 2{\it d}) & ({\it n}, 1024, 2{\it d}) \\
        ReLU & & ({\it n}, 1024, 2{\it d}) \\
        Trans Conv1D (Stride=4) & (25, 2{\it d}, {\it d}) & ({\it n}, 4096, {\it d}) \\
        ReLU & & ({\it n}, 4096, {\it d}) \\
        Trans Conv1D (Stride=4) & (25, {\it d}, {\it c}) & ({\it n}, 16384, {\it d}) \\
        Tanh & & ({\it n}, 16384, {\it d}) \\

      \end{tabular}
    \end{center}
    
  \end{table}

  \begin{table}[h]
    
    \caption{\label{tab:table-name} WaveGAN Generator for downsampled data}
    
    \begin{center}
      \begin{tabular}{ l | l | l}
        
        Operation & Kernel Size & Output Shape \\
        \hline
        Input {\it z} $\sim$ Uniform(-1, 1) & & ({\it n}, 100) \\
        Dense 1 & (100, 256{\it d}) & ({\it n}, 256{\it d}) \\
        Reshape & & ({\it n}, 16, 16{\it d}) \\
        ReLU & & ({\it n}, 16, 16{\it d}) \\
        Trans Conv1D (Stride=4) & (25, 16{\it d}, 8{\it d}) & ({\it n}, 64, 8{\it d}) \\
        ReLU & & ({\it n}, 64, 8{\it d}) \\
        Trans Conv1D (Stride=4) & (25, 8{\it d}, 4{\it d}) & ({\it n}, 256, 4{\it d}) \\
        ReLU & & ({\it n}, 256, 4{\it d}) \\
        Trans Conv1D (Stride=4) & (25, 4{\it d}, 2{\it d}) & ({\it n}, 1024, 2{\it d}) \\
        ReLU & & ({\it n}, 1024, 2{\it d}) \\
        Trans Conv1D (Stride=4) & (25, 2{\it d}, {\it d}) & ({\it n}, 4096, {\it d}) \\
        Tanh & & ({\it n}, 4096, {\it d}) \\

      \end{tabular}
    \end{center}
    
  \end{table}
  
  \begin{table}[h]
    
    \caption{\label{tab:table-name} WaveGAN Discriminator}
    
    \begin{center}
      \begin{tabular}{ l | l | l}
        
        Operation & Kernel Size & Output Shape \\
        \hline
        Input {\it x} or {\it G}({\it z}) & & ({\it n}, 16384, {\it c}) \\
        % to 4096
        Conv1D (Stride=4) & (25, {\it c}, {\it d}) & ({\it n}, 4096, {\it d}) \\
        LReLU (\textalpha \, = 0.2) & & ({\it n}, 4096, {\it d}) \\
        Phase Shuffle ({\it n} = 2) & & ({\it n}, 4096, {\it d}) \\
        % to 1024
        Conv1D (Stride=4) & (25, {\it d}, 2{\it d}) & ({\it n}, 1024, 2{\it d}) \\
        LReLU (\textalpha \, = 0.2) & & ({\it n}, 1024, 2{\it d}) \\
        Phase Shuffle ({\it n} = 2) & & ({\it n}, 1024, 2{\it d}) \\
        % to 256
        Conv1D (Stride=4) & (25, 2{\it d}, 4{\it d}) & ({\it n}, 256, 4{\it d}) \\
        LReLU (\textalpha \, = 0.2) & & ({\it n}, 256, 4{\it d}) \\
        Phase Shuffle ({\it n} = 2) & & ({\it n}, 256, 4{\it d}) \\
        % to 64
        Conv1D (Stride=4) & (25, 4{\it d}, 8{\it d}) & ({\it n}, 64, 8{\it d}) \\
        LReLU (\textalpha \, = 0.2) & & ({\it n}, 64, 8{\it d}) \\
        Phase Shuffle ({\it n} = 2) & & ({\it n}, 64, 8{\it d}) \\
        % to 16
        Conv1D (Stride=4) & (25, 8{\it d}, 16{\it d}) & ({\it n}, 16, 16{\it d}) \\
        LReLU (\textalpha \, = 0.2) & & ({\it n}, 16, 16{\it d}) \\
        % Output
        Reshape & & ({\it n}, 256{\it d}) \\
        Dense & (256{\it d}, 1) & ({\it n}, 1)\\
        
      \end{tabular}
    \end{center}
    
  \end{table}

  \begin{table}[h]
    
    \caption{\label{tab:table-name} Wave GAN Discriminator for downsampled data}
    
    \begin{center}
      \begin{tabular}{ l | l | l}
        
        Operation & Kernel Size & Output Shape \\
        \hline
        Input {\it x} or {\it G}({\it z}) & & ({\it n}, 4096, {\it c}) \\
        % to 1024
        Conv1D (Stride=4) & (25, {\it d}, 2{\it d}) & ({\it n}, 1024, 2{\it d}) \\
        LReLU (\textalpha \, = 0.2) & & ({\it n}, 1024, 2{\it d}) \\
        Phase Shuffle ({\it n} = 2) & & ({\it n}, 1024, 2{\it d}) \\
        % to 256
        Conv1D (Stride=4) & (25, 2{\it d}, 4{\it d}) & ({\it n}, 256, 4{\it d}) \\
        LReLU (\textalpha \, = 0.2) & & ({\it n}, 256, 4{\it d}) \\
        Phase Shuffle ({\it n} = 2) & & ({\it n}, 256, 4{\it d}) \\
        % to 64
        Conv1D (Stride=4) & (25, 4{\it d}, 8{\it d}) & ({\it n}, 64, 8{\it d}) \\
        LReLU (\textalpha \, = 0.2) & & ({\it n}, 64, 8{\it d}) \\
        Phase Shuffle ({\it n} = 2) & & ({\it n}, 64, 8{\it d}) \\
        % to 16
        Conv1D (Stride=4) & (25, 8{\it d}, 16{\it d}) & ({\it n}, 16, 16{\it d}) \\
        LReLU (\textalpha \, = 0.2) & & ({\it n}, 16, 16{\it d}) \\
        % Output
        Reshape & & ({\it n}, 256{\it d}) \\
        Dense & (256{\it d}, 1) & ({\it n}, 1)\\
        
      \end{tabular}
    \end{center}
    
  \end{table}
  
\end{appendices}

% Wrapup
\end{document}
